# Setup The Environment

## Load The Packages

```{r}
library(readstata13)
library(tidyverse)
library(ggplot2)
library(caret)
library(randomForest)
library(lfe)
library(grf)

# Install packages "causalLearning" from source
# We will use the causalBoosting and causalMARS in this package
install.packages("ranger")
library(ranger)
install.packages("causalLearning_1.0.0.tar.gz", repo=NULL, type="source")
# Load the package
library(causalLearning)
```

# Data Preparation And Exploration

## Load The Datasets

```{r}
endlines <- read.dta13("data/2013-0533_data_endlines1and2.dta",
                       generate.factors = T)
```

## The Structure And Summary

```{r}
str(endlines)
```

```{r}
colnames(endlines)
```

## Convert Factors to Binary Variable

```{r}
for (covar in colnames(endlines)) {
  if (is.factor(endlines[, covar]) == TRUE) {
    endlines[, covar] <- as.numeric(endlines[, covar]) - 1
  }
}
```

## Split Endline1 And Endline2

```{r}
endline1 <- endlines %>%
  filter(sample1 == 1) %>%
  select(colnames(endlines)[1:16], contains("_1"),
         -c("w", "w1", "w2", "sample1", "sample2"))
```

```{r}
str(endline1)
```


## Missing Values

```{r}
endline1$girl515_workhrs_pc_1 <- NULL
endline1$boy515_workhrs_pc_1 <- NULL
endline1$girl1620_school_1 <- NULL
endline1$boy1620_school_1 <- NULL
endline1$girl515_school_1 <- NULL
endline1$boy515_school_1 <- NULL
endline1$female_biz_pct_1 <- NULL
endline1$hours_child1620_week_1 <- NULL
endline1$hours_girl1620_week_1 <- NULL
endline1$hours_boy1620_week_1 <- NULL
endline1$biz_stop_1 <- NULL
endline1$biz_index_new_1 <- NULL
endline1$biz_index_old_1 <- NULL
```

### Fill with Median
```{r}
for (covar in colnames(endline1)) {
  if (is.numeric(endline1[, covar]) == TRUE) {
    endline1[is.na(endline1[, covar]), covar] <- median(endline1[, covar], 
                                                        na.rm = TRUE)
  }
}
sapply(endline1, function(x) sum(is.na(x)))
```

## Merge the Date

```{r}
endline1$visitdate <- as.Date(with(endline1, paste(visityear_1, visitmonth_1, visitday_1,sep="-")), "%Y-%m-%d")
str(endline1$visitdate)
summary(endline1$visitdate)
```

```{r}
endline1$visitday_1 <- NULL
endline1$visitmonth_1 <- NULL
endline1$visityear_1 <- NULL
endline1$visitdate <- NULL
endline1$w1 <- NULL
endline1$w2 <- NULL
```

# Design Of The Study

We want to study the effect of "availibility of microcredit" on different aspects of the households in Hyderabad, India: 

- Business
- ...

However, the fact that in the original study, they didn't collect the baseline data in a very rigorous way and they were not confident enough that the baseline data is representative of the slum of whole.  Hence the baseline data was only as a basis for stratification, the descriptive analysis, and to collect **area-level characteristics** that are used as control variables.

Because of the flaw of our datasets, we lose the ability to directly link baseline data with endlines data, hence could not perform the analysis on household-level. To mitigate this issue, we use the "index variables", which were calculated by the authors and were included in our dataset, as our target variables. And we assumed that those variables already include the information we need to analyze the causal effect.

### How They Calculate The Results in The Original Paper

For each "target" variable, they run an weighted OLS:

$y_{ia} = \alpha + \beta * Treatment_a + X'\gamma + \epsilon_{ia}$

```{r}
spandana_amt <- felm(spandana_amt_1 ~ 1 + treatment + area_pop_base + 
                     area_business_total_base + area_exp_pc_mean_base + 
                     area_literate_head_base + area_literate_base + 
                     area_debt_total_base, 
                     data = endline1,
                     weights = endline1$w1)

summary(spandana_amt)
```

```{r}
endline1 %>%
  filter(treatment == "Control") %>%
  summarize(mean = mean(spandana_amt_1, na.rm = TRUE))
```

# Business Index

## Treatment & Target Variable

- Treatment Variable:  `treatment`
- Target Variable: `biz_index_all_1`

We want to find out whether there are heterogeneous effects of  "availibility of Spandana microcredit loan" on business in the area.

```{r}
endline1 %>%
  filter(is.na(treatment) == FALSE) %>% # exclude the observations with NA
  group_by(treatment) %>%
  summarize("Num. of Obs." = n(),
            "Ave. Biz. Index" = mean(biz_index_all_1, na.rm = TRUE))
```

## Dataset

### TO DO any_biz_1?
### TO DO exclude indexes?

```{r}
target_index <- "biz_index_all_1"

endline1_biz <- endline1 %>%
  filter(any_biz_1 == 1) %>%
  select(everything(),
         -any_biz_1,
#         -contains("index"),                              # prevent confounding
         target_index)                                    # add target index
str(endline1_biz)
```

## Sorted Group Treatment Effects (GATES)

### Step 00: Setup Environment

Setup the following variables:

- Number of Simulations: 1 (for now)
- Number of Groups: 5 (for now)
- Treatment: `treatment`
- Treatment Assignment: randomized assignment
- Target Variable: `biz_index_all_1`

```{r}
set.seed(12345)
sim <- 100
groups <- 5
```

### Step 01: Split the data into Auxiliary and Main samples

```{r}
auxi_ind <- createDataPartition(dataset$treatment, p = 0.7, list = FALSE)
auxi <- dataset[auxi_ind, ]
main <- dataset[-auxi_ind,]
```

```{r}
str(auxi)
```

### Step 02: Train Two Models

In this section we use auxiliary sample to train two models:
- $Y_i=\hat{g_0}(X_i,D_i=0)+\hat{U_i}$
- $Y_i=\hat{g_0}(X_i,D_i=1)+\hat{U_i}$

```{r}
# seperate treatment & control in the auxiliary sample
auxi_treat_ind <- which(auxi$treatment == 1)
auxi_treat <- auxi[auxi_treat_ind, ]
auxi_contr <- auxi[-auxi_treat_ind,]
```

```{r}
auxi_yi0 <- randomForest(biz_index_all_1 ~ . - hhid - areaid - w1,
                         data = auxi_contr,
                         ntree = 3000, 
                         mtry = 3, 
                         replace = TRUE,
                         type = "regression")
auxi_yi1 <- randomForest(biz_index_all_1 ~ . - hhid - areaid - w1,
                         data = auxi_treat,
                         ntree = 3000,
                         mtry = 3,
                         replace = TRUE,
                         type = "regression")
```

### Step 03: Train The Propensity Score Model

Because we assumed the treatment was assigned randomly, the propensity score would be constant for all observations.

We use the auxiliary sample to train a model for propensity score.
- $D_i=\hat{m_0}(X_i)+\hat{V_i}$

```{r}
prop_score <- nrow(dataset[dataset$treatment == 1, ])/nrow(dataset)
```

```{r}
auxi_d <- randomForest(treatment ~ . -biz_index_all_1 -hhid -w1,
                       data = auxi,
                       ntree = 1000, 
                       mtry = 3, 
                       replace = TRUE,
                       type = "regression")
prop_score <- predict(auxi_d, newdata = main)
```

### Step 04: Estimate Conditional Baseline And Treatment Function

We predict the conditonal baseline and conditional treatment effect for each observation in main sample using the models we train in Step02.

```{r}
main_yi0 <- predict(auxi_yi0, newdata = main)
main_yi1 <- predict(auxi_yi1, newdata = main)
main$baseline <- main_yi0
main$condi_treat <- (main_yi1 - main_yi0)
```

### Step 05: Divide Observations Into Groups

```{r}
breaks <- quantile(main$condi_treat, seq(0,1, 1/groups), include.lowest = TRUE)
breaks[1] <- breaks[1] - 0.001
breaks[6] <- breaks[6] + 0.001
main$treat_group <- cut(main$condi_treat, breaks = breaks)
```

### Step 06: Estimate The Propensity Score

```{r}
main$prop_score <- prop_score
```

### Step 07: Calculate Propensity Score Offset

Prepare to perform the weighted OLS.
$\sum_{k}\gamma_k*(D-p(X))*1(G_k)+v$

First we calculate the propensity score offset $D-p(X)$. Then multiply with $1(G_k)$ to construct a dataframe that will later be used in the OLS formula.

```{r}
# calculate the propensity score offset for each observation in main sample
main$prop_offset <- main$treatment - main$prop_score
summary(main$prop_offset)
```

```{r}
# construct matrix from each observation's group factor
SGX <- model.matrix(~-1+main$treat_group)
# construct D-p(X)*1(G_k) for each observation
DSG <- data.frame(main$prop_offset*SGX)
colnames(DSG) <- c("G1", "G2", "G3", "G4", "G5")
head(DSG)
```

### Step 08: Run Weighted OLS

$Y=\alpha'X_1\sum_{g}\gamma_g*(D-p(X))*1(G_g)+v$

```{r}
main[,c("G1", "G2", "G3", "G4", "G5")] <- cbind(
  DSG$G1, DSG$G2, DSG$G3, DSG$G4, DSG$G5)
```

### Without Clustering

```{r}
gates <- felm(biz_index_all_1~-1+baseline+condi_treat+G1+G2+G3+G4+G5|0|0|0,
              data = maine)
summary(gates)
```

### With Clustering

```{r}
gates_cl<- felm(biz_index_all_1~baseline+condi_treat+G1+G2+G3+G4+G5|0|0|areaid,
                data = main,
                weights = main$w1)
summary(gates_cl)
```

```{r}
main %>% 
  mutate(groups = case_when(
      G1 != 0 ~ "G1",
      G2 != 0 ~ "G2",
      G3 != 0 ~ "G3",
      G4 != 0 ~ "G4",
      G5 != 0 ~ "G5",
      TRUE ~ "unknown")) %>%
  group_by(groups) %>%
  summarize(bis_expense = mean(bizexpense_1),
            informal_amt = mean(informal_amt_1),
            spandana_amt = mean(spandana_amt_1),
            bis_profit = mean(bizprofit_1))
```

## Simulation

```{r}
gates_results <- list()
gates_cluster_results <- list()

for (i in 1:sim) {
  seed <- i + floor(runif(1, min = 0, max = 10000))
  set.seed(seed)

  auxi_ind <- createDataPartition(dataset$treatment, p = 0.7, list = FALSE)
  auxi <- dataset[auxi_ind, ]
  main <- dataset[-auxi_ind,]

  # seperate treatment & control in the auxiliary sample
  auxi_treat_ind <- which(auxi$treatment == 1)
  auxi_treat <- auxi[auxi_treat_ind, ]
  auxi_contr <- auxi[-auxi_treat_ind,]

  auxi_yi0 <- randomForest(biz_index_all_1 ~ . - hhid - areaid - w1,
                           data = auxi_contr,
                           ntree = 1000,
                           mtry = 3,
                           replace = TRUE,
                           type = "regression")
  auxi_yi1 <- randomForest(biz_index_all_1 ~ . - hhid - areaid - w1,
                           data = auxi_treat,
                           ntree = 1000,
                           mtry = 3,
                           replace = TRUE,
                           type = "regression")

  # prop_score <- nrow(dataset[dataset$treatment == 1, ])/nrow(dataset)

  auxi_d <- randomForest(treatment ~ . -biz_index_all_1 -hhid -w1,
                         data = auxi,
                         ntree = 1000,
                         mtry = 3,
                         replace = TRUE,
                         type = "regression")
  prop_score <- predict(auxi_d, newdata = main)

  main_yi0 <- predict(auxi_yi0, newdata = main)
  main_yi1 <- predict(auxi_yi1, newdata = main)
  main$baseline <- main_yi0
  main$condi_treat <- (main_yi1 - main_yi0)

  breaks <- quantile(main$condi_treat, seq(0,1, 1/groups), include.lowest = TRUE)
  breaks[1] <- breaks[1] - 0.001
  breaks[6] <- breaks[6] + 0.001
  main$treat_group <- cut(main$condi_treat, breaks = breaks)

  main$prop_score <- prop_score

  # calculate the propensity score offset for each observation in main sample
  main$prop_offset <- main$treatment - main$prop_score

  # construct matrix from each observation's group factor
  SGX <- model.matrix(~-1+main$treat_group)
  # construct D-p(X)*1(G_k) for each observation
  DSG <- data.frame(main$prop_offset*SGX)
  colnames(DSG) <- c("G1", "G2", "G3", "G4", "G5")

  main[,c("G1", "G2", "G3", "G4", "G5")] <- cbind(
    DSG$G1, DSG$G2, DSG$G3, DSG$G4, DSG$G5)

  gates <- felm(biz_index_all_1~baseline+condi_treat+G1+G2+G3+G4+G5|0|0|0,
                data = main,
                weights = main$w1)

  gates_cl <- felm(biz_index_all_1~baseline+condi_treat+G1+G2+G3+G4+G5|0|0|areaid,
                   data = main,
                   weights = main$w1)

  gates_results[[i]] <- gates
  gates_cluster_results[[i]] <- gates_cl
}
```

```{r}
gates_summary <- data.frame(matrix(nrow = 2, ncol = 7))
coefs <- c("baseline", "condi_treat", "G1", "G2", "G3", "G4", "G5")
colnames(gates_summary) <- coefs

for (coef in coefs) {
  gates_summary[1, coef] <- median(sapply(gates_results, 
                                          function(x) x$coefficients[coef, ]))
  gates_summary[2, coef] <- mean(sapply(gates_cluster_results,
                                          function(x) x$coefficients[coef, ]))
}
```

```{r}
gates_summary[1, ] # median
```

```{r}
gates_summary[2, ] # mean
```

# CAUSAL FOREST

## Var_imp_plot function
```{r}
var_imp_plot <- function(forest, decay.exponent = 2L, max.depth = 4L) {
  
  # Calculate variable importance of all features
  # (from print.R)
  split.freq <- split_frequencies(forest, max.depth)
  split.freq <- split.freq / pmax(1L, rowSums(split.freq))
  weight <- seq_len(nrow(split.freq)) ^ -decay.exponent
  var.importance <- t(split.freq) %*% weight / sum(weight)
  
  # Format data frame
  p <- ncol(forest$X.orig)
  
  var.names <- colnames(forest$X.orig)[seq_len(p)]
  if (is.null(var.names)) {
    var.names <- paste0('x', seq_len(p))
  }
  df <- tibble(Variable = var.names,
               Importance = as.numeric(var.importance)) %>%
    arrange(Importance) %>% 
    mutate(Variable = factor(Variable, levels = unique(Variable)))
  
  # Plot results
  p <- ggplot(df, aes(Variable, Importance)) + 
    geom_bar(stat = 'identity') + 
    coord_flip() + 
    ggtitle('Variable Importance') + 
    theme_bw() + 
    theme(plot.title = element_text(hjust = 0.5))
  print(p)
}
```

## Business Index
```{r}
target_index <- "biz_index_all_1"

endline1_biz <- endline1 %>%
  filter(any_biz_1 == 1) %>%
  select(everything(),
         -any_biz_1)  
str(endline1_biz)

y <- endline1_biz$biz_index_all_1
X <- endline1_biz %>% 
  select(
    everything(), -treatment, -hhid,
-contains("index"), -contains("biz"), -contains("area"), 
-hhsize_1, -adults_1, children_1 
)
clusters <- endline1_biz$areaid
W <- endline1_biz$treatment

set.seed(123)
forest_cluster <- causal_forest(
  model.matrix(~., data = X),
  y,
  W,
  clusters = clusters,
  mtry = 1, num.trees = 1000)
forest_cluster

var_imp_plot(forest_cluster)
```

## Credit Index
```{r}
target_index <- "credit_index_1"

y <- endline1$credit_index_1
X <- endline1 %>% 
  select(
    everything(), -treatment, -hhid, -areaid,
-contains("index"), -contains("area"), -contains("amt"), -contains("mfi"), 
-spandana_1, -anyloan_1, -anybank_1, -anyinformal_1, -everlate_1,
-hhsize_1, -adults_1, -children_1 
)
clusters <- endline1$areaid
W <- endline1$treatment

set.seed(123)
forest_without_cluster <- causal_forest(
  model.matrix(~., data = X),
  y,
  W,
  clusters = clusters,
  mtry = 1, num.trees = 1000)
forest_cluster

var_imp_plot(forest_cluster)
```

## Home Durable Index
```{r}
target_index <- "home_durable_index_1"

y <- endline1$home_durable_index_1
X <- endline1 %>% 
  select(
    everything(), -treatment, -hhid, -areaid,
-contains("index"), -contains("area"), -contains("exp"), bizexpense_1, 
-hhsize_1, -adults_1, -children_1 
)
clusters <- endline1$areaid
W <- endline1$treatment

str(X)
set.seed(123)
forest_cluster <- causal_forest(
  model.matrix(~., data = X),
  y,
  W,
  clusters = clusters,
  mtry = 1, num.trees = 1000)
forest_cluster

var_imp_plot(forest_cluster)
```

## Consumption Index
```{r}
target_index <- "consumption_index_1"

y <- endline1$consumption_index_1
X <- endline1 %>% 
  select(
    everything(), -treatment, -hhid, -areaid,
-contains("index"), -contains("area"), -contains("exp"), bizexpense_1, 
-hhsize_1, -adults_1, -children_1 
)
clusters <- endline1$areaid
W <- endline1$treatment

str(X)
set.seed(123)
forest_cluster <- causal_forest(
  model.matrix(~., data = X),
  y,
  W,
  clusters = clusters,
  mtry = 1, num.trees = 1000)
forest_cluster

var_imp_plot(forest_cluster)
```

```{r}
index <- endline1 %>%
select(contains("index"))
str(index)
