---
output:
  pdf_document: default
  html_document: default
---
# Setup The Environment

## Load The Required Packages

```{r}
library(readstata13)
library(tidyverse)
library(outliers)
library(ggplot2)
library(caret)
library(randomForest)
library(lfe)
library(grf)
library(cowplot)
```

## Source The Helper Functions

```{r}
source("helpers.r")
```

# Data Preparation And Exploration

## Load The Datasets

```{r}
endlines <- read.dta13("data/2013-0533_data_endlines1and2.dta",
                       convert.factors = FALSE,
                       generate.factors = TRUE)
str(endlines)
```

## Split Endline1 And Endline2

```{r}
endline1 <- endlines %>%
  filter(sample1 == 1) %>%
  select(colnames(endlines)[1:16], contains("_1"))
str(endline1)
```

## Exclude Irrelevant & Redundant Covariates

There are some variables in the dataset that are only relevant when the data were collected, such as the information of when the inspectors visit the households and if the households were included in the endline surveys.

```{r}
endline1 <- endline1 %>%
  select(-c(w, w1, w2, sample1, sample2, visitday_1, visitmonth_1, visityear_1))
```

Since we are going to use `areaid` to do cluster analysis, including the area-level variables doesn't make much sense.

```{r}
endline1 <- endline1 %>%
  select(-starts_with("area_"))
```

The dataset include both total expense and per-capita version for each expense category per month (or annual). To prevent issues with overspecified (irrelevant variables), we exclude the total expenses and only leave the per-capita variables.

```{r}
endline1 <- endline1 %>%
  select(-ends_with("_mo_1"),
         -ends_with("_annual_1"))
```

### Business-related Variables

`old_biz` and `any_old_biz` contain similar information, the former indicates how many old businesses a household own prior to the first endline and the latter is a binary variable that indicates whether a household has at least an old business. Here we combine the two variables and assume those households that didn't answer the question was either not a busniess owner at all or not understood the question. Either ways, we could safely consider them as having 0 old busineses.

```{r}
summary(endline1$old_biz)
summary(endline1$any_old_biz)
```

```{r}
endline1 <- endline1 %>%
  mutate(old_biz = ifelse(any_old_biz == 0 | is.na(any_old_biz) == TRUE,
                          0,
                          old_biz))
# Delete any_old_biz as the information is combined with old_biz
endline1$any_old_biz <- NULL
```

The same reasoning could be apply to `total_biz_1` and `any_biz_1`.

```{r}
endline1 <- endline1 %>%
  mutate(total_biz_1 = ifelse(any_biz_1 == 0 | is.na(any_biz_1) == TRUE,
                              0,
                              total_biz_1))
# Delete any_biz_1 as the information is combined with total_biz_1
endline1$any_biz_1 <- NULL
```

```{r}
endline1 <- endline1 %>%
  mutate(newbiz_1 = ifelse(any_new_biz_1 == 0 | is.na(any_new_biz_1) == TRUE,
                           0,
                           newbiz_1))
# Delete any_biz_1 as the information is combined with newbiz_1
endline1$any_new_biz_1 <- NULL
```

### Household-related variables

```{r}
endline1 %>% 
  select(hhsize_1, adults_1, children_1) %>% 
  mutate(hh_total = adults_1 + children_1) %>% 
  filter(hhsize_1 != hh_total) %>%
  nrow()
```

```{r}
endline1$hhsize_1 <- NULL
```

### Loan-related Variables

```{r}
endline1 %>% 
  select(anymfi_1, spandana_1, othermfi_1) %>% 
  mutate(mfi = max(spandana_1, othermfi_1)) %>% 
  filter(anymfi_1 != mfi) %>%
  nrow()

endline1 %>% 
  select(anymfi_amt_1, spandana_amt_1, othermfi_amt_1) %>% 
  mutate(total_mfi_amt = spandana_amt_1 + othermfi_amt_1) %>% 
  filter(anymfi_amt_1 != total_mfi_amt) %>%
  nrow()
```

```{r}
endline1$anymfi_1 <- NULL
endline1$anymfi_amt_1 <- NULL
```

### Labor-related Variables

```{r}
endline1 %>% 
  select(hours_week_1, hours_week_biz_1, hours_week_outside_1) %>% 
  mutate(hours_week_sum = hours_week_biz_1 + hours_week_outside_1) %>% 
  filter(hours_week_1 != hours_week_sum) %>%
  nrow()

endline1 %>% 
  select(hours_headspouse_week_1, hours_headspouse_biz_1, hours_headspouse_outside_1) %>% 
  mutate(hours_headspouse_week_sum = hours_headspouse_biz_1 + hours_headspouse_outside_1) %>% 
  filter(hours_headspouse_week_1 != hours_headspouse_week_sum) %>%
  nrow()

endline1 %>% 
  select(hours_child1620_week_1, hours_boy1620_week_1, hours_girl1620_week_1) %>% 
  mutate(hours_children_total = hours_boy1620_week_1 + hours_girl1620_week_1) %>% 
  filter(hours_child1620_week_1 != hours_children_total) %>%
  nrow()
```

```{r}
endline1$hours_week_1 <- NULL
endline1$hours_headspouse_week_1 <- NULL
endline1$hours_child1620_week_1 <- NULL
```

## Missing Values

We will first delete the covariates that contains a huge amount of missing values. Then we will look into the remaining covariates and fill them with custom methods.

First we need to find out which variable contains unreasonable amount of missing value.

```{r}
# set the threshold of na ratio
na_delete_threshold <- 0.1
na_table(endline1) %>% filter(ratio > na_delete_threshold)
```

We will delete those variables as the information might not be helpful.

```{r}
# select the variables that has a large amount of missing value
na_delete_col <- (na_table(endline1) %>% filter(ratio > na_delete_threshold))[,1]
# delete those variables
for (col in na_delete_col) {
  endline1[,col] <- NULL
}
```

### Filling The Missing Values - Business-related Variables

```{r}
endline1 <- endline1 %>% 
  mutate(bizassets_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                              0,
                              bizassets_1),
         bizinvestment_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                                  0,
                                  bizinvestment_1),
         bizrev_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                           0,
                           bizrev_1),
         bizexpense_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                               0,
                               bizexpense_1),
         bizprofit_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                              0,
                              bizprofit_1),
         bizemployees_1 = ifelse(total_biz_1 == 0 | is.na(total_biz_1),
                                 0,
                                 bizemployees_1))
```

### For All The Other Variables Except Index Variables

```{r}
covariates_name <- endline1 %>% 
  select(-contains("index")) %>%
  colnames()
for (covar in covariates_name) {
  endline1[is.na(endline1[, covar]), covar] <- 
    median(endline1[, covar], na.rm = TRUE)
}
```

### Check The Result

```{r}
na_table(endline1) %>% filter(n != 0)
```

```{r}
endline1 <- na.omit(endline1)
nrow(endline1)
```

## Outliers

First we want to know which column (variable) contains outliers and how many of them. Here we will use "Z-score" approach to detect outliers.

```{r}
exp_col <- endline1 %>% 
<<<<<<< HEAD:colab_notebook.Rmd
  select(contains("consumption_index_1")) %>%
  colnames()
for (covar in exp_col) {
  covar_outlier <- scores(x = endline1[, covar], type = "iqr", lim = 10)
=======
  select(contains("exp_mo_pc")) %>%
  colnames()
for (covar in exp_col) {
  covar_outlier <- scores(x = endline1[, covar], type = "iqr", lim = 5)
>>>>>>> f85a5f3681b39436cb717a86a8949ed91497cdb0:notebook.Rmd
  endline1 <- endline1[!covar_outlier, ]
}
```

# Design Of The Study

We want to study the effect of "availibility of microcredit" on different aspects of the households in Hyderabad, India: 

- Business
- ...

However, the fact that in the original study, they didn't collect the baseline data in a very rigorous way and they were not confident enough that the baseline data is representative of the slum of whole.  Hence the baseline data was only as a basis for stratification, the descriptive analysis, and to collect **area-level characteristics** that are used as control variables.

Because of the flaw of our datasets, we lose the ability to directly link baseline data with endlines data, hence could not perform the analysis on household-level. To mitigate this issue, we use the "index variables", which were calculated by the authors and were included in our dataset, as our target variables. And we assumed that those variables already include the information we need to analyze the causal effect.

## How They Calculate The Results in The Original Paper

For each "target" variable, they run an weighted OLS:

$y_{ia} = \alpha + \beta * Treatment_a + X'\gamma + \epsilon_{ia}$

```{r eval=FALSE, include=FALSE}
spandana_amt <- felm(spandana_amt_1 ~ 1 + treatment + area_pop_base + 
                     area_business_total_base + area_exp_pc_mean_base + 
                     area_literate_head_base + area_literate_base + 
                     area_debt_total_base, 
                     data = endline1,
                     weights = endline1$w1)

summary(spandana_amt)
```

```{r}
endline1 %>%
  filter(treatment == "Control") %>%
  summarize(mean = mean(spandana_amt_1, na.rm = TRUE))
```

# Business Index

## Treatment & Target Variable

- Treatment Variable:  `treatment`
- Target Variable: `biz_index_all_1`

We want to find out whether there are heterogeneous effects of  "availibility of Spandana microcredit loan" on business in the area.

```{r}
endline1 %>%
  group_by(treatment) %>%
  summarize("Num. of Obs." = n(),
            "Ave. Biz. Index" = mean(biz_index_all_1, na.rm = TRUE))
```

## Define The Dataset

```{r}
target_index <- "biz_index_all_1"

endline1_biz <- endline1 %>%
  select(everything(),
         -hhid,
         -starts_with("biz"),
         -contains("index"), # prevent confounding
         target_index)       # add target index
str(endline1_biz)
```

## Two-Models Approach With Sorted Group Average Treatment Effects (GATES)

<<<<<<< HEAD:colab_notebook.Rmd
## F-Tests
```{r eval=FALSE, include=FALSE}
# auxi_ind <- createDataPartition(y = endline1$treatment, p = 0.7, list = FALSE)
=======
### F-Tests
```{r}
auxi_ind <- createDataPartition(y = endline1$treatment, p = 0.7, list = FALSE)
>>>>>>> f85a5f3681b39436cb717a86a8949ed91497cdb0:notebook.Rmd
auxi <- endline1[auxi_ind, ]
main <- endline1[-auxi_ind,]

auxi_treat_ind <- which(auxi[,"treatment"] == 1)
auxi_treat <- auxi[auxi_treat_ind, ]
auxi_contr <- auxi[-auxi_treat_ind,]

var.test(auxi_treat$spouse_literate_1, auxi_contr$spouse_literate_1, alternative = "two.sided")
var.test(auxi_treat$spouse_works_wage_1, auxi_contr$spouse_works_wage_1, alternative = "two.sided")
var.test(auxi_treat$hhsize_1, auxi_contr$hhsize_1, alternative = "two.sided")
var.test(auxi_treat$women1845_1, auxi_contr$women1845_1, alternative = "two.sided")
var.test(auxi_treat$anychild1318_1, auxi_contr$anychild1318_1, alternative = "two.sided")
var.test(auxi_treat$old_biz, auxi_contr$old_biz, alternative = "two.sided")
var.test(auxi_treat$ownland_hyderabad_1, auxi_contr$ownland_hyderabad_1, alternative = "two.sided")
var.test(auxi_treat$ownland_village_1, auxi_contr$ownland_village_1, alternative = "two.sided")

for (covar in colnames(auxi_treat)) {
  F_test <- var.test(auxi_treat[,covar], auxi_contr[,covar], alternative = "two.sided")
  signif_value <- 0.05
  print(ifelse(F_test$p.value < 0.05, covar, 0))
  }

treated <- endline1 %>%
  filter(treatment == 1)

control <- endline1 %>%
  filter(treatment == 0)
 
var.test(treated$spouse_literate_1, control$spouse_literate_1, alternative = "two.sided")
var.test(treated$spouse_works_wage_1, control$spouse_works_wage_1, alternative = "two.sided")
var.test(treated$hhsize_1, control$hhsize_1, alternative = "two.sided")
var.test(treated$women1845_1, control$women1845_1, alternative = "two.sided")
var.test(treated$anychild1318_1, control$anychild1318_1, alternative = "two.sided")
var.test(treated$old_biz, control$old_biz, alternative = "two.sided")
var.test(treated$ownland_hyderabad_1, control$ownland_hyderabad_1, alternative = "two.sided")
var.test(treated$ownland_village_1, control$ownland_village_1, alternative = "two.sided")

for (covar in colnames(treated)) {
  F_test <- var.test(treated[,covar], control[,covar], alternative = "two.sided")
  signif_value <- 0.05
  print(ifelse(F_test$p.value < 0.05, covar, 0))
  }

# The p-value of F-test is greater than the significance level 0.05. In conclusion, there is no significant difference between the two variances.

```

### Rescale The Dataset

We rescale (standardize) the numeric variables before running the
regression in order to get a clear view on how to interpret the
results.

```{r}
preScale <- preProcess(endline1_biz[,3:(ncol(endline1_biz)-1)],
                       method = c("scale", "center"))

endline1_biz_scale <- predict(preScale, newdata = endline1_biz)
````

### Results (using Random Forest)

```{r}
tm_gates_biz <- tm_gates("biz_index_all_1", "treatment", endline1_biz_scale,
                         split_ratio = 0.6,
                         cluster="areaid", num.iter=1, ml_method="rf")
```

```{r}
summary(tm_gates_biz[[1]])
```

```{r}
summary(tm_gates_biz[[2]])
```

### Results (using Causal Random Forest)

```{r}
tm_gates_biz_crf <- tm_gates("biz_index_all_1", "treatment", endline1_biz_scale,
                             split_ratio = 0.6,
                             cluster="areaid", num.iter=1, ml_method="crf")
```

```{r}
summary(tm_gates_biz_crf[[1]])
```

```{r}
summary(tm_gates_biz_crf[[2]])
```

<<<<<<< HEAD:colab_notebook.Rmd
# CAUSAL FOREST

## Var_imp_plot function
```{r}
var_imp_plot <- function(forest, decay.exponent = 2L, max.depth = 4L) {
  
  # Calculate variable importance of all features
  # (from print.R)
  split.freq <- split_frequencies(forest, max.depth)
  split.freq <- split.freq / pmax(1L, rowSums(split.freq))
  weight <- seq_len(nrow(split.freq)) ^ -decay.exponent
  var.importance <- t(split.freq) %*% weight / sum(weight)
  
  # Format data frame
  p <- ncol(forest$X.orig)
  
  var.names <- colnames(forest$X.orig)[seq_len(p)]
  if (is.null(var.names)) {
    var.names <- paste0('x', seq_len(p))
  }
  df <- tibble(Variable = var.names,
               Importance = as.numeric(var.importance)) %>%
    arrange(Importance) %>% 
    mutate(Variable = factor(Variable, levels = unique(Variable)))
  
  # Plot results
  p <- ggplot(df, aes(Variable, Importance)) + 
    geom_bar(stat = 'identity') + 
    coord_flip() + 
    ggtitle('Variable Importance') + 
    theme_bw() + 
    theme(plot.title = element_text(hjust = 0.5))
  print(p)
}
```

## Trend Plots Function

```{r}
trend_plots <- function(crf, test) {
  # Get the variable importance table
  var_imp <- crf %>% 
    variable_importance() %>% 
    as.data.frame() %>% 
    mutate(variable = colnames(crf$X.orig)) %>% 
    arrange(desc(V1))
  # for the first four most important variable
  # create a plot that shows if there are trend of correlation
  p1 <- ggplot(test, aes(x = test[, var_imp$variable[1]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[1], y = "pred. CTE")
  p2 <- ggplot(test, aes(x = test[, var_imp$variable[2]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[2], y = "pred. CTE")
  p3 <- ggplot(test, aes(x = test[, var_imp$variable[3]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[3], y = "pred. CTE")
  p4 <- ggplot(test, aes(x = test[, var_imp$variable[4]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[4], y = "pred. CTE")
  p5 <- ggplot(test, aes(x = test[, var_imp$variable[5]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[5], y = "pred. CTE")
  p6 <- ggplot(test, aes(x = test[, var_imp$variable[6]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[6], y = "pred. CTE")
  p7 <- ggplot(test, aes(x = test[, var_imp$variable[7]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[7], y = "pred. CTE")
  p8 <- ggplot(test, aes(x = test[, var_imp$variable[8]], y = preds)) +
    geom_point() +
    geom_smooth(method = "loess", span = 1) +
    theme_light() +
    labs(x = var_imp$variable[8], y = "pred. CTE")
  # combine those plots
  cowplot::plot_grid(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 2)
}
```
```{r}
endline1 %>%
  select(
    contains("amt")
  )
```


## Business Index

```{r}
target_index <- "biz_index_all_1"

endline1_biz <- endline1 %>%
  filter(total_biz_1 != 0) %>%
  select(everything(),
         -hhid,
         -contains("biz"),
         -total_exp_mo_pc_1,
         -contains("index"),
         # prevent confounding
         target_index)       # add target index
str(endline1_biz)

endline1_biz$biz_index_all_1 <- ifelse(endline1_biz$biz_index_all_1<0, 0, 1)
# test/train
set.seed(123)
idx.train <- caret::createDataPartition(y = endline1_biz$treatment, p = 0.75, list = FALSE) 
train <- endline1_biz[idx.train, ] # training set
test <-  endline1_biz[-idx.train, ]

# train data
Y <- train$biz_index_all_1
X <- train %>% 
  select(-treatment, -target_index, -areaid)
X.clusters <- train$areaid
W <- train$treatment

# model
forest <- causal_forest(
  model.matrix(~., data = X),
  Y,
  W,
  clusters = X.clusters,
  mtry = 3, 
  num.trees = 3000,
  honesty = TRUE)

var_imp_plot(forest)

# test data
test_Y <- test$biz_index_all_1
test_X <- test %>% 
  select(-treatment, -target_index, -areaid)
test_clusters <- test$areaid
test_W <- test$treatment

# prediction
preds <- predict(
  object = forest,
  newdata = model.matrix(~ ., data = test_X, estimate.variance = TRUE))
test$preds <- preds$predictions
trend_plots(forest, test)
```

```{r}
test <- cbind(test, preds)
table <- QiniTable(test, "treatment", "biz_index_all_1", "predictions")
table
QiniCurve(table)
```

```{r}
target_index <- "biz_index_all_1"

endline1_biz <- endline1[1:15]
biz_index_all_1 <- endline1$biz_index_all_1
endline1_biz <- cbind(endline1_biz, biz_index_all_1)


# test/train
set.seed(123)
idx.train <- caret::createDataPartition(y = endline1_biz$treatment, p = 0.75, list = FALSE) 
train <- endline1_biz[idx.train, ] # training set
test <-  endline1_biz[-idx.train, ]

# train data
Y <- train$biz_index_all_1
X <- train %>% 
  select(-treatment, -target_index, -areaid, -hhid)
X.clusters <- train$areaid
W <- train$treatment

# model
forest <- causal_forest(
  model.matrix(~., data = X),
  Y,
  W,
  clusters = X.clusters,
  mtry = 3, 
  num.trees = 3000,
  honesty = TRUE)

var_imp_plot(forest)

# test data
test_Y <- test$biz_index_all_1
test_X <- test %>% 
  select(-treatment, -target_index, -areaid)
test_clusters <- test$areaid
test_W <- test$treatment

# prediction
preds <- predict(
  object = forest,
  newdata = model.matrix(~ ., data = test_X, estimate.variance = TRUE))
test$preds <- preds$predictions
trend_plots(forest, test)
```


```{r}
str(endline1_biz)
```

```{r}
Y.forest = regression_forest(X, Y, clusters = X.clusters)
Y.hat = predict(Y.forest)$predictions
W.forest = regression_forest(X, W, clusters = X.clusters)
W.hat = predict(W.forest)$predictions

cf.raw = causal_forest(X, Y, W,
                       Y.hat = Y.hat, W.hat = W.hat,
                       clusters = X.clusters)

varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp))

cf = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.hat,
                   clusters = X.clusters,
                   samples_per_cluster = 10,
                   tune.parameters = TRUE)

tau.hat = predict(cf)$predictions
```

### Confidence Interval for Average Treatment Effects

```{r}
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect) 
ate.low = average_treatment_effect(cf, subset = !high_effect) 

paste("95% CI for difference in ATE:",
round(ate.high[1] - ate.low[1], 3), "+/-",
round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))
```

### Best Linear Predictor

```{r}
test_calibration(cf)
```
Heterogeneity with 0.1 significance.

### The Effects of `hours_week_biz_1` and `total_exp_mo_pc_1`

```{r}
area.mat = model.matrix(~ -1 + areaid, data = train)
area.size = colSums(area.mat)

dr.score = tau.hat + W / cf$W.hat * (Y - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W) / (1 - cf$W.hat) * (Y - cf$Y.hat + cf$W.hat * tau.hat)

area.score = area.mat * dr.score / area.size
```


```{r}
area.hours_biz = area.mat * X$hours_week_biz_1/ area.size
high_hours_biz = area.hours_biz > median(area.hours_biz) 
t.test(area.score[high_hours_biz], area.score[!high_hours_biz])
```


```{r}
area.total_exp = area.mat * X$total_exp_mo_pc_1 / area.size
high_total_exp = area.total_exp > median(area.total_exp) 
t.test(area.score[high_total_exp], area.score[!high_total_exp])
```

```{r}
area.food_exp = area.mat * X$food_exp_mo_pc_1 / area.size
high.food_exp = area.food_exp > median(area.food_exp) 
t.test(area.score[high.food_exp], area.score[!high.food_exp]) 
```
There is heterogeneity along `hours_week_biz_1` and `total_exp_mo_pc_1` with 0.05 significance.



## Credit Index
```{r}
target_index <- "credit_index_1"

endline1_credit <- endline1 %>%
  select(everything(),
         -hhid,
         -spandana_1,
         -othermfi_1,
         -anybank_1,
         -anyinformal_1,
         -anyloan_1,
         -contains("amt"),
         -contains("index"), # prevent confounding
         target_index)       # add target index
str(endline1_credit)

endline1_biz$biz_index_all_1 <- ifelse(endline1_biz$biz_index_all_1<0, 0, 1)

# test/train
set.seed(123)
idx.train <- caret::createDataPartition(y = endline1_credit$treatment, p = 0.75, list = FALSE) 
train <- endline1_credit[idx.train, ] # training set
test <-  endline1_credit[-idx.train, ]

# train data
Y <- train$credit_index_1
X <- train %>% 
  select(-treatment, -target_index, -areaid)
X.clusters <- train$areaid
W <- train$treatment

# model
forest <- causal_forest(
  model.matrix(~., data = X),
  Y,
  W,
  clusters = X.clusters,
  mtry = 3, 
  num.trees = 3000,
  honesty = TRUE)

var_imp_plot(forest)

# test data
test_Y <- test$credit_index_1
test_X <- test %>% 
  select(-treatment, -target_index, -areaid)
test_clusters <- test$areaid
test_W <- test$treatment

# prediction
preds <- predict(
  object = forest,
  newdata = model.matrix(~ ., data = test_X, estimate.variance = TRUE))
test$preds <- preds$predictions
trend_plots(forest, test)
```

```{r}
test <- cbind(test, preds)
table <- QiniTable(test, "treatment", "credit_index_all_1", "predictions")
table
QiniCurve(table)
```

```{r}
Y.forest = regression_forest(X, Y, clusters = X.clusters)
Y.hat = predict(Y.forest)$predictions
W.forest = regression_forest(X, W, clusters = X.clusters)
W.hat = predict(W.forest)$predictions

cf.raw = causal_forest(X, Y, W,
                       Y.hat = Y.hat, W.hat = W.hat,
                       clusters = X.clusters)

varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp))

cf = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.hat,
                   clusters = X.clusters,
                   samples_per_cluster = 10,
                   tune.parameters = TRUE)

tau.hat = predict(cf)$predictions
```

### Confidence Interval for Average Treatment Effects

```{r}
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect) 
ate.low = average_treatment_effect(cf, subset = !high_effect) 

paste("95% CI for difference in ATE:",
round(ate.high[1] - ate.low[1], 3), "+/-",
round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))
```

### Best Linear Predictor

```{r}
test_calibration(cf)
```
There is heterogeneity.

### The Effects of `total_exp_mo_pc_1` and `food_exp_mo_pc_1` and `nondurable_exp_mo_pc_1`

```{r}
area.mat = model.matrix(~ -1 + areaid, data = train)
area.size = colSums(area.mat)

dr.score = tau.hat + W / cf$W.hat * (Y - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W) / (1 - cf$W.hat) * (Y - cf$Y.hat + cf$W.hat * tau.hat)

area.score = area.mat * dr.score / area.size
```

```{r}
area.total_exp = area.mat * X$total_exp_mo_pc_1 / area.size
high_total_exp = area.total_exp > median(area.total_exp) 
t.test(area.score[high_total_exp], area.score[!high_total_exp])
```

The t-test shows no significant of different treatment effects between high food expenditure and 

```{r}
area.food_exp = area.mat * X$food_exp_mo_pc_1 / area.size
high.food_exp = area.food_exp > median(area.food_exp) 
t.test(area.score[high.food_exp], area.score[!high.food_exp]) 
```
```{r}
area.nondurable_exp = area.mat * X$nondurable_exp_mo_pc_1 / area.size
high.nondurable_exp = area.nondurable_exp > median(area.nondurable_exp) 
t.test(area.score[high.nondurable_exp], area.score[!high.nondurable_exp]) 
```
There is heterogeneity along `total_exp_mo_pc_1` and `food_exp_mo_pc_1` and `nondurable_exp_mo_pc_1`.


## Consumption Index
```{r}
target_index <- "consumption_index_1"

endline1_consumption <- endline1 %>%
  select(everything(),
         -hhid,
         -contains("exp"),
         bizexpense_1,
         -contains("index"), # prevent confounding
         target_index)       # add target index
str(endline1_consumption)


# test/train
set.seed(123)
idx.train <- caret::createDataPartition(y = endline1_consumption$treatment, p = 0.75, list = FALSE) 
train <- endline1_consumption[idx.train, ] # training set
test <-  endline1_consumption[-idx.train, ]

# train data
Y <- train$consumption_index_1
X <- train %>% 
  select(-treatment, -target_index, -areaid)
X.clusters <- train$areaid
W <- train$treatment

# model
forest <- causal_forest(
  model.matrix(~., data = X),
  Y,
  W,
  clusters = X.clusters,
  mtry = 3, 
  num.trees = 3000,
  honesty = TRUE)

var_imp_plot(forest)

# test data
test_Y <- test$consumption_index_1
test_X <- test %>% 
  select(-treatment, -target_index, -areaid)
test_clusters <- test$areaid
test_W <- test$treatment

# prediction
preds <- predict(
  object = forest,
  newdata = model.matrix(~ ., data = test_X, estimate.variance = TRUE))
test$preds <- preds$predictions
trend_plots(forest, test)
```




```{r}
Y.forest = regression_forest(X, Y, clusters = X.clusters)
Y.hat = predict(Y.forest)$predictions
W.forest = regression_forest(X, W, clusters = X.clusters)
W.hat = predict(W.forest)$predictions

cf.raw = causal_forest(X, Y, W,
                       Y.hat = Y.hat, W.hat = W.hat,
                       clusters = X.clusters)

varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp))

cf = causal_forest(X[,selected.idx], Y, W,
                   Y.hat = Y.hat, W.hat = W.hat,
                   clusters = X.clusters,
                   samples_per_cluster = 10,
                   tune.parameters = TRUE)

tau.hat = predict(cf)$predictions
```

### Confidence Interval for Average Treatment Effects

```{r}
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect) 
ate.low = average_treatment_effect(cf, subset = !high_effect) 

paste("95% CI for difference in ATE:",
round(ate.high[1] - ate.low[1], 3), "+/-",
round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))
```

### Best Linear Predictor

```{r}
test_calibration(cf)
```
No heterogeneity. 

### The Effects of `anyloan_amt_1` and `hhsize_adj_1`

```{r}
area.mat = model.matrix(~ -1 + areaid, data = train)
area.size = colSums(area.mat)

dr.score = tau.hat + W / cf$W.hat * (Y - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W) / (1 - cf$W.hat) * (Y - cf$Y.hat + cf$W.hat * tau.hat)

area.score = area.mat * dr.score / area.size
```

```{r}
area.anyloan_amt = area.mat * X$anyloan_amt_1 / area.size
high_anyloan_amt = area.anyloan_amt > median(area.anyloan_amt) 
t.test(area.score[high_anyloan_amt], area.score[!high_anyloan_amt])
```

```{r}
area.hhsize_adj = area.mat * X$hhsize_adj_1 / area.size
high.hhsize_adj = area.hhsize_adj > median(area.hhsize_adj) 
t.test(area.score[high.hhsize_adj], area.score[!high.hhsize_adj]) 
```
No heterogeneity along `anyloan_amt_1` and `hhsize_adj_1`. 
